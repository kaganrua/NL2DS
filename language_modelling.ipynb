{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NL2DS-F2021-Assignment-6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# 1. Language Modeling\n",
        "\n",
        "In this part, let's generate text using a trigram language model.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1pR0koayRSgXfTD72HZUHN14uec0SrnXy?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a21d08-f62f-43af-a004-fdbb1fefbe1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see these folders.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a90145-6f6c-458f-9fde-42a7cc008379"
      },
      "source": [
        "!ls \"/content/drive/My Drive/nl2ds\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semantic-parser  tweets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Y7I_9lPoZS"
      },
      "source": [
        "Let's load the trigrams first. You can change the below code as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk"
      },
      "source": [
        "from math import log\n",
        "\n",
        "bigram_prefix_to_trigram = {}\n",
        "bigram_prefix_to_trigram_weights = {}\n",
        "\n",
        "lines = open(\"/content/drive/My Drive/nl2ds/tweets/covid-tweets-2020-08-10-2020-08-21.trigrams.txt\").readlines()\n",
        "for line in lines:\n",
        "  word1, word2, word3, count = line.strip().split()\n",
        "  if (word1, word2) not in bigram_prefix_to_trigram:\n",
        "    bigram_prefix_to_trigram[(word1, word2)] = []\n",
        "    bigram_prefix_to_trigram_weights[(word1, word2)] = []\n",
        "  bigram_prefix_to_trigram[(word1, word2)].append(word3)\n",
        "  bigram_prefix_to_trigram_weights[(word1, word2)].append(int(count))\n",
        "\n",
        "# freeup memory\n",
        "lines = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(bigram_prefix_to_trigram_weights[('middle' , 'of')])\n",
        "#print(bigram_prefix_to_trigram[('middle' , 'of')])\n",
        "count = 0\n",
        "\n",
        "for n in bigram_prefix_to_trigram_weights[('middle' , 'of')]:\n",
        "  count += n\n",
        "\n",
        "l1 = bigram_prefix_to_trigram[('middle' , 'of')]\n",
        "l2 = bigram_prefix_to_trigram_weights[('middle' , 'of')]\n",
        "\n",
        "i = 0\n",
        "for word,c in zip(l1,l2):\n",
        "  if i == 10:\n",
        "    break;\n",
        "  prob = c/count\n",
        "  print(word, prob)\n",
        "  i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyGMB8GhcPoC",
        "outputId": "829f5278-bbfa-456f-e6a4-8227250d48dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 0.807981220657277\n",
            "the 0.06948356807511737\n",
            "pandemic 0.023943661971830985\n",
            "this 0.016901408450704224\n",
            "an 0.0107981220657277\n",
            "covid 0.009389671361502348\n",
            "nowhere 0.008450704225352112\n",
            "it 0.004694835680751174\n",
            "lockdown 0.002347417840375587\n",
            "summer 0.002347417840375587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X48i3rarPzd8"
      },
      "source": [
        "## Problem 1.1: Retrieve top next words and their probability given a bigram prefix.\n",
        "\n",
        "For the following prefixes **word1=middle, word2=of, and n=10**, the output is:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "a 0.807981220657277\n",
        "the 0.06948356807511737\n",
        "pandemic 0.023943661971830985\n",
        "this 0.016901408450704224\n",
        "an 0.0107981220657277\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYhal88xSYow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e684cb-94ec-4134-ecfb-385fe01df366"
      },
      "source": [
        "def top_next_word(word1, word2, n=10):\n",
        "  next_words = []\n",
        "  probs = []\n",
        "  count = 0\n",
        "  for n1 in bigram_prefix_to_trigram_weights[(word1, word2)]:\n",
        "    count += n1\n",
        "  \n",
        "  i = 0\n",
        "  for word,c in zip(bigram_prefix_to_trigram[(word1,word2)], bigram_prefix_to_trigram_weights[(word1, word2)]):\n",
        "    if i == n:\n",
        "      break\n",
        "    prob = c/count\n",
        "    probs.append(prob)\n",
        "    next_words.append(word)\n",
        "    i += 1\n",
        "  \n",
        "  return next_words, probs\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "next_words, probs = top_next_word(\"middle\", \"of\", 10)\n",
        "for word, prob in zip(next_words, probs):\n",
        "  print(word, prob)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 0.807981220657277\n",
            "the 0.06948356807511737\n",
            "pandemic 0.023943661971830985\n",
            "this 0.016901408450704224\n",
            "an 0.0107981220657277\n",
            "covid 0.009389671361502348\n",
            "nowhere 0.008450704225352112\n",
            "it 0.004694835680751174\n",
            "lockdown 0.002347417840375587\n",
            "summer 0.002347417840375587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gok10i2dSHXB"
      },
      "source": [
        "## Problem 1.2: Sampling n words\n",
        "\n",
        "Sample next n words given a bigram prefix. Use the probablity distribution defined by the frequency counts. Functions like **numpy.random.choice** will be useful here. Sample without repitition, otherwise all your samples will contain the most frequent trigram.\n",
        "\n",
        "\n",
        "For the following prefixes **word1=middle, word2=of, and n=10**, the output could be as follows (our outputs may differ): \n",
        "\n",
        "```\n",
        "a 0.807981220657277\n",
        "pandemic 0.023943661971830985\n",
        "nowhere 0.008450704225352112\n",
        "the 0.06948356807511737\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OzYJoYfUaom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3384d96-e1bd-4b72-ad84-54cf4a366bd0"
      },
      "source": [
        "import numpy as np\n",
        "def sample_next_word(word1, word2, n=10):\n",
        "  next_words = bigram_prefix_to_trigram[(word1, word2)]\n",
        "  probs = bigram_prefix_to_trigram_weights[(word1, word2)]\n",
        "\n",
        "  total_count = 0\n",
        "  for prob in probs:\n",
        "    total_count += prob\n",
        "  \n",
        "  final_probs = []\n",
        "  for prob in probs:\n",
        "    final_probs.append(prob/total_count)\n",
        "    \n",
        "  #gave error during last part of q1, added for that intent\n",
        "  if len(next_words) < n:\n",
        "      n = len(next_words)\n",
        "  sampled_words = np.random.choice(next_words, n, replace=False, p=final_probs)  \n",
        "  new_probs = []\n",
        "  \n",
        "  for sample in sampled_words:\n",
        "    for word,prob in zip(next_words, probs):\n",
        "      if sample == word:\n",
        "        new_probs.append(prob/total_count)\n",
        "        continue\n",
        "\n",
        "  \n",
        "  return sampled_words, new_probs\n",
        "\n",
        "next_words, probs = sample_next_word(\"middle\", \"of\", 10)\n",
        "for word, prob in zip(next_words, probs):\n",
        "  print(word, prob)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 0.807981220657277\n",
            "oh 0.00046948356807511736\n",
            "pandemic 0.023943661971830985\n",
            "fifth 0.00046948356807511736\n",
            "covid 0.009389671361502348\n",
            "the 0.06948356807511737\n",
            "an 0.0107981220657277\n",
            "nowhere 0.008450704225352112\n",
            "#covid 0.00046948356807511736\n",
            "this 0.016901408450704224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYenU8H-fIR"
      },
      "source": [
        "## Problem 1.3: Generate sentences starting with a prefix\n",
        "\n",
        "Generates n-sentences starting with a given sentence prefix. Use [beam search](https://en.wikipedia.org/wiki/Beam_search) to generate multiple sentences. Depending on which method you use to generate next word, you will get different outputs. When you generate <EOS> in a path, stop exploring that path. If you are not careful with your implementation, you may end up in an infinite loop.\n",
        "\n",
        "If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output is as follows:\n",
        "```\n",
        "<BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n",
        "<BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n",
        "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output is as follows:\n",
        "```\n",
        "<BOS1> <BOS2> biden calls for a 30 bonus URL #cashgem #cashappfriday #stayathome <EOS> 0.0002495268686322749\n",
        "<BOS1> <BOS2> biden says all u.s. governors should mandate masks <EOS> 1.6894510541025754e-05\n",
        "<BOS1> <BOS2> biden says all u.s. governors question cost of a pandemic <EOS> 8.777606198953028e-07\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output may look as follows (since this is sampling, our outputs will difer):\n",
        "\n",
        "```\n",
        "<BOS1> <BOS2> trump signs executive orders URL <EOS> 7.150992253427233e-05\n",
        "<BOS1> <BOS2> trump signs executive actions URL <EOS> 7.117242889600614e-05\n",
        "<BOS1> <BOS2> trump news president attacked over it <EOS> 1.0546494007903964e-05\n",
        "<BOS1> <BOS2> trump news president attacked over executive orders URL <EOS> 1.0126405114118984e-05\n",
        "```\n",
        "\n",
        "If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output may look as follows:\n",
        "\n",
        "```\n",
        "<BOS1> <BOS2> biden harris 2020 <EOS> 0.0015758924114719264\n",
        "<BOS1> <BOS2> biden harris 2020 URL <EOS> 0.0006443960952032196\n",
        "<BOS1> <BOS2> biden calls for evictions ban so marylander 's do it URL <EOS> 4.105215709355001e-07\n",
        "<BOS1> <BOS2> biden calls for evictions ban so marylander 's do our best to stay home <EOS> 1.3158806336098573e-09\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "Hope you see that sampling gives different outputs compared to deterministically picking the top n-words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40kW0joweXFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea56160d-9a8b-436e-f428-f611dd3a2e00"
      },
      "source": [
        "import heapq\n",
        "\n",
        "\n",
        "def generate_sentences(prefix, sampler, beam=10):\n",
        "  sentence = prefix.split()\n",
        "\n",
        "\n",
        "\n",
        "  start_beam = [(1.0 , sentence , False)]\n",
        "  \n",
        "\n",
        "  while 1:\n",
        "    new_beam = []\n",
        "    for score, sentence,boolean in start_beam:\n",
        "      end = sentence[-1]\n",
        "\n",
        "      if end == '<EOS>':\n",
        "        boolean = True\n",
        "        heapq.heappush(new_beam, (score, sentence, boolean))\n",
        "        if len(new_beam) > beam:\n",
        "          heapq.heappop(new_beam)\n",
        "\n",
        "      else:\n",
        "        next_words, probs = sampler(sentence[-2], sentence[-1] , n=beam)\n",
        "      \n",
        "        for word, prob in zip(next_words, probs):\n",
        "          new_score = prob * score\n",
        "          heapq.heappush(new_beam, (new_score, sentence + [word] , False) )\n",
        "          if len(new_beam) > beam:\n",
        "            heapq.heappop(new_beam)\n",
        "    start_beam = new_beam\n",
        "    all_done = True\n",
        "    for (_,_,boolean) in start_beam:\n",
        "      if boolean == False:\n",
        "        all_done = False\n",
        "    \n",
        "    if all_done:\n",
        "      break\n",
        "\n",
        "  sorted_beam = sorted(start_beam, key = lambda tup: tup[0] , reverse=True)\n",
        "\n",
        "  sentences = []\n",
        "  new_probs = []\n",
        "  for s in sorted_beam:\n",
        "    sentences.append(s[1])\n",
        "    new_probs.append(s[0])\n",
        "   \n",
        "  \n",
        "  \n",
        "    \n",
        "  \n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return sentences, new_probs\n",
        "\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=top_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=top_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=sample_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=sample_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'coronavirus', 'treatment', 'URL', '<EOS>'] 0.00021893147502903603\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'coronavirus', 'cure', 'URL', '<EOS>'] 0.0001719607222046247\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'mypillow', 'ceo', 'over', 'unproven', 'therapeutic', 'URL', '<EOS>'] 9.773272077557522e-05\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'coronavirus', 'therapeutic', 'mypillow', 'creator', 'over', 'unproven', 'therapeutic', 'URL', '<EOS>'] 8.212549111137046e-05\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'mypillow', 'ceo', 'over', 'unproven', 'therapeutic', 'URL', 'via', '@USER', '<EOS>'] 7.432226908194607e-06\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'mypillow', 'ceo', 'over', 'unproven', 'and', 'dangerous', 'covid-19', 'treatment', 'URL', '<EOS>'] 5.235550241426875e-06\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'mypillow', 'ceo', 'over', 'unproven', 'and', 'dangerous', 'URL', '<EOS>'] 4.625645250343987e-06\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'ben', 'carson', 'and', 'mypillow', 'founder', 'and', 'ceo', 'of', 'mypillow', 'URL', '<EOS>'] 2.1484173056680325e-06\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'ben', 'carson', 'and', 'mypillow', 'founder', 'and', 'ceo', 'mike', 'lindell', 'a', 'big', 'deal', '<EOS>'] 1.4263568494891567e-08\n",
            "['<BOS1>', '<BOS2>', 'trump', 'eyes', 'new', 'unproven', 'virus', 'cure', 'promoted', 'by', 'ben', 'carson', 'and', 'mypillow', 'founder', 'and', 'ceo', 'mike', 'lindell', 'a', 'big', 'deal', 'and', 'the', 'pandemic', '<EOS>'] 4.890594996717161e-12\n",
            "#########################\n",
            "\n",
            "['<BOS1>', '<BOS2>', 'biden', 'calls', 'for', 'a', '30', 'bonus', 'URL', '#cashgem', '#cashappfriday', '#stayathome', '<EOS>'] 0.0002495268686322749\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'question', 'cost', 'of', 'a', 'pandemic', '<EOS>'] 8.777606198953028e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'rule', 'URL', '<EOS>'] 6.46094976762742e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'masks', 'and', 'social', 'distancing', '<EOS>'] 4.6833316176693136e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'masks', 'and', 'social', 'distancing', 'URL', '<EOS>'] 2.3817382579228685e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'wearing', 'and', 'social', 'distancing', '<EOS>'] 4.380454675651422e-08\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'wearing', 'and', 'social', 'distancing', 'URL', '<EOS>'] 2.2277082512658355e-08\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'wearing', 'and', 'social', 'distancing', 'and', 'wearing', 'masks', '<EOS>'] 1.166766912614153e-10\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'wearing', 'and', 'social', 'distancing', 'and', 'wearing', 'a', 'mask', '<EOS>'] 1.1355525098965152e-10\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'wearing', 'and', 'social', 'distancing', 'and', 'wearing', 'a', 'mask', 'and', 'social', 'distancing', 'and', 'masks', 'are', 'not', 'the', 'same', 'time', '<EOS>'] 2.3660719518591428e-20\n",
            "#########################\n",
            "\n",
            "['<BOS1>', '<BOS2>', 'trump', 'says', 'children', 'unlikely', 'to', 'catch', 'covid', '<EOS>'] 8.084122922908798e-06\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'a', 'pandemic', '<EOS>'] 3.793481382459569e-07\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'this', 'pandemic', '<EOS>'] 3.364134187751416e-07\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'the', 'middle', 'of', 'a', 'pandemic', 'is', 'over', '<EOS>'] 2.592416655394151e-09\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'for', 'covid-19', 'URL', '<EOS>'] 1.687823010398221e-09\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'URL', '<EOS>'] 9.75850763244866e-10\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'for', 'coronavirus', 'URL', '<EOS>'] 4.287324572340275e-10\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'for', 'coronavirus', '<EOS>'] 2.3208996143862027e-10\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'for', 'covid-19', 'URL', 'URL', '<EOS>'] 2.1914339095969416e-10\n",
            "['<BOS1>', '<BOS2>', 'trump', 'calls', 'out', 'university', 'leadership', 'on', 'in', 'person', 'classes', 'after', '130', 'students', 'test', 'positive', 'for', 'covid-19', 'URL', 'via', '@USER', '<EOS>'] 1.4531915115590514e-10\n",
            "#########################\n",
            "\n",
            "['<BOS1>', '<BOS2>', 'biden', 'calls', 'for', 'a', '30', 'bonus', 'URL', '#cashgem', '#cashappfriday', '#stayathome', '<EOS>'] 0.0002495268686322749\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'masks', '<EOS>'] 1.6894510541025754e-05\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'women', 'amp', 'children', '<EOS>'] 1.5046191808853178e-05\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'question', 'cost', 'of', 'a', 'pandemic', '<EOS>'] 8.777606198953028e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'masks', 'and', 'social', 'distancing', 'URL', '<EOS>'] 2.3817382579228685e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'orders', 'read', 'more', 'URL', '<EOS>'] 1.7187340911698745e-07\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'orders', 'read', 'more', 'URL', 'URL', '<EOS>'] 4.331340864876274e-08\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'orders', 'arrest', 'of', 'rev.', 'robert', 'jones', '#dyetthungerstriker', 'today', '4', 'peacefully', 'protesting', '”', 'rioting', 'or', 'looting', 'or', 'riots', 'spreading', 'the', 'virus', 'URL', '<EOS>'] 8.584864605944798e-14\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'orders', 'arrest', 'of', 'rev.', 'robert', 'jones', '#dyetthungerstriker', 'today', '4', 'peacefully', 'protesting', '”', 'rioting', 'or', 'looting', 'or', 'riots', 'spreading', 'the', 'coronavirus', 'pandemic', 'URL', '<EOS>'] 1.6031457448347598e-14\n",
            "['<BOS1>', '<BOS2>', 'biden', 'says', 'all', 'u.s.', 'governors', 'should', 'mandate', 'mandatory', 'mask', 'orders', 'arrest', 'of', 'rev.', 'robert', 'jones', '#dyetthungerstriker', 'today', '4', 'peacefully', 'protesting', '”', 'rioting', 'or', 'looting', 'or', 'riots', 'spreading', 'the', 'coronavirus', 'pandemic', 'URL', 'URL', '<EOS>'] 1.481536615353685e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UShw7ULDcOwU"
      },
      "source": [
        "# 2. Semantic Parsing\n",
        "\n",
        "In this part, you are going to build your own virtual assistant! We will be developing two modules: an intent classifier and a slot filler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj2nWqZ9dZUs",
        "outputId": "1b111bf4-ab31-49c9-d3a7-32efc4e59601"
      },
      "source": [
        "!ls \"/content/drive/My Drive/nl2ds/semantic-parser\"\n",
        "parser_files = \"/content/drive/My Drive/nl2ds/semantic-parser\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_answers.txt  test_questions.txt  train_questions_answers.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbtOC6eecMNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be98d226-b048-4bb7-e305-5b2677b981c7"
      },
      "source": [
        "import json\n",
        "\n",
        "train_data = []\n",
        "for line in open(f'{parser_files}/train_questions_answers.txt'):\n",
        "    train_data.append(json.loads(line))\n",
        "\n",
        "# print a few examples\n",
        "for i in range(5):\n",
        "    print(train_data[i])\n",
        "    print(\"-\"*80)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Add an album to my Sylvia Plath playlist.', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'album', 'playlist_owner': 'my', 'playlist': 'Sylvia Plath'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add Diarios de Bicicleta to my la la playlist', 'intent': 'AddToPlaylist', 'slots': {'playlist': 'Diarios de Bicicleta', 'playlist_owner': 'my', 'entity_name': 'la la'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'book a table at a restaurant in Lucerne Valley that serves chicken nugget', 'intent': 'BookRestaurant', 'slots': {'restaurant_type': 'restaurant', 'city': 'Lucerne Valley', 'served_dish': 'chicken nugget'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add iemand als jij to my playlist named In The Name Of Blues', 'intent': 'AddToPlaylist', 'slots': {'entity_name': 'iemand als jij', 'playlist_owner': 'my', 'playlist': 'In The Name Of Blues'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'What will the weather be in the current position on Dec. 23?', 'intent': 'GetWeather', 'slots': {'current_location': 'current position', 'timeRange': 'Dec. 23'}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMV-NkkAb6X3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5268e934-5ca7-49a8-e570-18603662fb4f"
      },
      "source": [
        "test_questions = []\n",
        "for line in open(f'{parser_files}/test_questions.txt'):\n",
        "    test_questions.append(json.loads(line))\n",
        "\n",
        "test_answers = []\n",
        "for line in open(f'{parser_files}/test_answers.txt'):\n",
        "    test_answers.append(json.loads(line))\n",
        "\n",
        "# print a few examples\n",
        "for i in range(5):\n",
        "    print(test_questions[i])\n",
        "    print(test_answers[i])\n",
        "    print(\"-\"*80)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add an artist to Jukebox Boogie Rhythm & Blues\n",
            "{'intent': 'AddToPlaylist', 'slots': {'music_item': 'artist', 'playlist': 'Jukebox Boogie Rhythm & Blues'}}\n",
            "--------------------------------------------------------------------------------\n",
            "Will it be rainy at Sunrise in Ramey Saudi Arabia?\n",
            "{'intent': 'GetWeather', 'slots': {'condition_description': 'rainy', 'timeRange': 'Sunrise', 'city': 'Ramey', 'country': 'Saudi Arabia'}}\n",
            "--------------------------------------------------------------------------------\n",
            "Weather in two hours  in Uzbekistan\n",
            "{'intent': 'GetWeather', 'slots': {'timeRange': 'in two hours', 'country': 'Uzbekistan'}}\n",
            "--------------------------------------------------------------------------------\n",
            "Will there be a cloud in VI in 14 minutes ?\n",
            "{'intent': 'GetWeather', 'slots': {'condition_description': 'cloud', 'state': 'VI', 'timeRange': 'in 14 minutes'}}\n",
            "--------------------------------------------------------------------------------\n",
            "add nuba to my Metal Party playlist\n",
            "{'intent': 'AddToPlaylist', 'slots': {'entity_name': 'nuba', 'playlist_owner': 'my', 'playlist': 'Metal Party'}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLUozTj613Tk"
      },
      "source": [
        "## Problem 2.1: Keyword-based intent classifier\n",
        "\n",
        "In this part, you will build a keyword-based intent classifier. For each intent, come up with a list of keywords that are important for that intent, and then classify a given question into an intent. If an input question matches multiple intents, pick the best one. If it does not match any keyword, return None.\n",
        "\n",
        "Caution: You are allowed to look at training questions and answers to come up with a set of keywords, but it is a bad practice to look at test answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIOcz3lC4VqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6866af0e-ed27-4cc6-a14a-c8d7e3b0a3bd"
      },
      "source": [
        "# List of all intents\n",
        "intents = set()\n",
        "for example in train_data:\n",
        "    intents.add(example['intent'])\n",
        "print(intents)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'AddToPlaylist', 'GetWeather', 'BookRestaurant'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at the training data for having a better idea of the keywords\n",
        "for i in range(20):\n",
        "    print(train_data[i])\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JotZuLrgmLL1",
        "outputId": "96c074a5-ed88-4d92-8272-f46d009992b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Add an album to my Sylvia Plath playlist.', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'album', 'playlist_owner': 'my', 'playlist': 'Sylvia Plath'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add Diarios de Bicicleta to my la la playlist', 'intent': 'AddToPlaylist', 'slots': {'playlist': 'Diarios de Bicicleta', 'playlist_owner': 'my', 'entity_name': 'la la'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'book a table at a restaurant in Lucerne Valley that serves chicken nugget', 'intent': 'BookRestaurant', 'slots': {'restaurant_type': 'restaurant', 'city': 'Lucerne Valley', 'served_dish': 'chicken nugget'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add iemand als jij to my playlist named In The Name Of Blues', 'intent': 'AddToPlaylist', 'slots': {'entity_name': 'iemand als jij', 'playlist_owner': 'my', 'playlist': 'In The Name Of Blues'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'What will the weather be in the current position on Dec. 23?', 'intent': 'GetWeather', 'slots': {'current_location': 'current position', 'timeRange': 'Dec. 23'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'What will the weather be in Virginia?', 'intent': 'GetWeather', 'slots': {'state': 'Virginia'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'I need a reservation for three people at a spa restaurant in two hundred thirty seven days', 'intent': 'BookRestaurant', 'slots': {'party_size_number': 'three', 'facility': 'spa', 'restaurant_type': 'restaurant', 'timeRange': 'in two hundred thirty seven days'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': \"What's the weather in Heritage Hill State Historical Park\", 'intent': 'GetWeather', 'slots': {'geographic_poi': 'Heritage Hill State Historical Park'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'Put dschiwan gasparjan into the Cool Down playlist. ', 'intent': 'AddToPlaylist', 'slots': {'artist': 'dschiwan gasparjan', 'playlist': 'Cool Down'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'Will it be colder in Åland ', 'intent': 'GetWeather', 'slots': {'condition_temperature': 'colder', 'country': 'Åland'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add the album to my Top 100 Indie Tracks on Spotify playlist', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'album', 'playlist_owner': 'my', 'playlist': 'Top 100 Indie Tracks on Spotify'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'Add the tune to my Epic Classical playlist.', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'tune', 'playlist_owner': 'my', 'playlist': 'Epic Classical'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'book a cafeteria for 5 in Turkmenistan', 'intent': 'BookRestaurant', 'slots': {'restaurant_type': 'cafeteria', 'party_size_number': '5', 'country': 'Turkmenistan'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'Add song to orgánica', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'song', 'playlist': 'orgánica'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'Will it rain in Paisley', 'intent': 'GetWeather', 'slots': {'condition_description': 'rain', 'city': 'Paisley'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'I want to book a cafeteria in El Reno that serves javanese', 'intent': 'BookRestaurant', 'slots': {'restaurant_type': 'cafeteria', 'city': 'El Reno', 'cuisine': 'javanese'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'What is the weather supposed to be like in New Jersey three months from now', 'intent': 'GetWeather', 'slots': {'state': 'New Jersey', 'timeRange': 'three months from now'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'I want to find parking next to a restaurant for 10', 'intent': 'BookRestaurant', 'slots': {'facility': 'parking', 'restaurant_type': 'restaurant', 'party_size_number': '10'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'What is the weather like right now in the same area as Georgia', 'intent': 'GetWeather', 'slots': {'timeRange': 'now', 'spatial_relation': 'in the same area', 'state': 'Georgia'}}\n",
            "--------------------------------------------------------------------------------\n",
            "{'question': 'add rock & roll to my playlist named night out', 'intent': 'AddToPlaylist', 'slots': {'entity_name': 'rock & roll', 'playlist_owner': 'my', 'playlist': 'night out'}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmpLWuCO46NG"
      },
      "source": [
        "def predict_intent_using_keywords(question):\n",
        "  #includes some music genres for better accuracy, will be tried without them as well\n",
        "  keywords_playlist = ['album' , 'Album' , 'add' , 'Add' , 'playlist'  , 'Playlist' , 'put' , 'Put' , 'tune' ,'Tune', 'song' , 'Song', 'Songs','songs', 'music' , 'Music' ,'musics','Musics', 'named' , 'rock & roll' , 'techno' , 'Techno' , 'Pop' , 'Rap' , 'pop' , 'rap' , 'Hip-Hop' , 'hiphop' , 'HipHop', 'indie' , 'Indie' , 'Electronic' , 'Organica' , 'organica' , 'track' , 'tracks' , 'Track' , 'Tracks']\n",
        "  keywords_weather = ['weather' , 'Weather' , 'What', 'position','Position', 'area', 'Area', 'rain', 'Rain' , 'snow' , 'Snow' , 'cold' , 'Cold' , 'colder' , 'Colder' , 'warm' , 'warmer' , 'Warm' , 'Warmer' , 'sun' , 'Sun' , 'Sunny' , 'sunny' , 'winter' , 'Winter' , 'Summer' , 'summer', 'heat' , 'Heat', 'wave' , 'Wave', 'Spring' , 'spring' , 'fall' , 'Fall',\n",
        "                      'Jan.' , 'Jan' , 'january' , 'January' ,'jan' , 'jan.' , \n",
        "                      'Feb.', 'feb.' , 'Feb' ,'feb' , 'February' , 'february' , \n",
        "                      'Mar.' , 'mar.' , 'Mar' , 'mar' , 'March' , 'march' ,\n",
        "                      'Apr.' , 'apr.' , 'Apr' , 'apr' , 'April' , 'april' , \n",
        "                      'May.' , 'may.' , 'May', 'may',\n",
        "                      'June' , 'june',\n",
        "                      'July' , 'july',\n",
        "                      'Aug.' , 'aug.' , 'Aug' , 'aug' , 'August' , 'august',\n",
        "                      'Sept.', 'sept.' , 'Sept' , 'sept', 'September' , 'september',\n",
        "                      'Oct.' , 'oct.' , 'Oct' , 'oct' , 'October' , 'october',\n",
        "                      'Nov.' , 'nov.' , 'Nov' , 'nov' , 'November' , 'november',\n",
        "                      'Dec.' , 'dec.' , 'Dec' , 'dec' , 'December' , 'december']\n",
        "  keywords_book = ['book' , 'Book' , 'table' , 'Table' , 'chicken' , 'Chicken' , 'beef' , 'Beef', 'meat' , 'Meat' , 'restaurant' , 'Restaurant' , 'cafe' , 'Cafe' , 'reservation' , 'Reservation' , 'reserve' , 'Reserve' ,'cafeteria', 'Cafeteria' , 'serve' , 'serves', 'Vine' , 'vine', 'coffee' , 'Coffee' , 'spring' , 'Spring' , 'Fall' , 'fall']\n",
        "\n",
        "  score_play = 0\n",
        "  score_weather = 0\n",
        "  score_book = 0\n",
        "  dummy=0\n",
        "  for token in question.split():\n",
        "    if token in keywords_playlist:\n",
        "      score_play += 1\n",
        "    if token in keywords_weather:\n",
        "      score_weather += 1\n",
        "    if token in keywords_book:\n",
        "      score_book += 1\n",
        "    \n",
        "  \n",
        "  if score_play > score_weather and score_play > score_book:\n",
        "    return 'AddToPlaylist'\n",
        "  if score_weather > score_play and score_weather > score_book:\n",
        "    return 'GetWeather'\n",
        "  if score_book > score_weather and score_book > score_play:\n",
        "    return 'BookRestaurant'\n",
        "  \n",
        "  if score_play == 0 and score_weather ==0 and score_book == 0:\n",
        "    return None\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSNHoHR16jk9",
        "outputId": "5fbaa208-9ef3-4426-de5c-d25977e6122c"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "'''Gives intent wise accuracy of your model'''\n",
        "def evaluate_intent_accuracy(prediction_function_name):\n",
        "  correct = Counter()\n",
        "  total = Counter()\n",
        "  for i in range(len(test_questions)):\n",
        "    q = test_questions[i]\n",
        "    gold_intent = test_answers[i]['intent']\n",
        "    if prediction_function_name(q) == gold_intent:\n",
        "      correct[gold_intent] += 1\n",
        "    total[gold_intent] += 1\n",
        "  for intent in intents:\n",
        "    print(intent, correct[intent]/total[intent], total[intent])\n",
        "    \n",
        "# Evaluating the intent classifier. \n",
        "# In our implementation, a simple keyword based classifier has achieved an accuracy of greater than 65 for each intent\n",
        "evaluate_intent_accuracy(predict_intent_using_keywords)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AddToPlaylist 1.0 100\n",
            "GetWeather 0.67 100\n",
            "BookRestaurant 0.95 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzV5NYJe-rbm"
      },
      "source": [
        "## Problem 2.2: Statistical intent classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jizp2fxbb6X5"
      },
      "source": [
        "Now, let's build a statistical intent classifier. Instead of making use of keywords like what you did above, you will first extract features from a given input question. In order to build a feature representation for a given sentence, make use of word2vec embeddings of each word and take an average to represent the sentence. Then train a logistic regression. Feel free to use any libraries you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPd4QZBhGOsm",
        "outputId": "55e8cf9c-76fd-42e8-e9ac-dd7c2dcf3e96"
      },
      "source": [
        "import nltk\n",
        "nltk.download('word2vec_sample')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iys8Cb3An3x"
      },
      "source": [
        "from nltk.data import find\n",
        "import gensim\n",
        "\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "Train_data = pd.DataFrame(train_data , columns=['question' , 'intent' , 'slots'] )\n",
        "\n",
        "tokenized_q = []\n",
        "i = 0\n",
        "\n",
        "for row in Train_data['question']:\n",
        "  tokenized_sentence = word_tokenize(row)\n",
        "  new_row = [str(word) for word in tokenized_sentence if word in word2vec_model.vocab]\n",
        "  \n",
        "  tokenized_q.append(new_row)\n",
        "  i += 1\n",
        "\n",
        "Train_data['tokenized_questions'] = tokenized_q\n",
        "#print(Train_data['tokenized_questions'].head(5))\n",
        "#print(tokenized_q[0:2])\n",
        "y_train = Train_data['intent']\n",
        "tokenized_q = np.array(tokenized_q)\n",
        "\n",
        "X_train = []\n",
        "for sentence in tokenized_q:\n",
        "  \n",
        "  vector_entry = word2vec_model[sentence]\n",
        "  X_train.append(np.mean(vector_entry, axis=0))\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "print(X_train)\n",
        "print(X_train[0].shape)\n",
        "print(X_train[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7syEsV7r0qB5",
        "outputId": "1c781807-26d6-4001-a2e4-408105505064"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.03452644  0.02010152 -0.02088016 ... -0.023274    0.05154817\n",
            "  -0.00663894]\n",
            " [ 0.01267474  0.02278275  0.04106298 ... -0.02427706 -0.00235562\n",
            "   0.01678254]\n",
            " [-0.01432118 -0.00722078  0.03015662 ... -0.00883467  0.01105457\n",
            "  -0.00851605]\n",
            " ...\n",
            " [ 0.05842185  0.01964904  0.00868735 ... -0.04953406  0.00836307\n",
            "   0.00071405]\n",
            " [ 0.01944842 -0.00018672  0.0240079  ...  0.00200638 -0.01975253\n",
            "  -0.02370804]\n",
            " [ 0.04823534  0.02462134 -0.00025777 ... -0.03358248  0.03556177\n",
            "  -0.0661042 ]]\n",
            "(300,)\n",
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpk3_JtMb6X6"
      },
      "source": [
        "'''Trains a logistic regression model on the entire training data. For an input question (x), the model learns to predict an intent (Y).'''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "\n",
        "def train_logistic_regression_intent_classifier():\n",
        "    lr = LogisticRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "    lr.trai\n",
        "    return lr\n"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1lHAw9b6X6"
      },
      "source": [
        "'''For an input question, the model predicts an intent'''\n",
        "model = train_logistic_regression_intent_classifier()\n",
        "def predict_intent_using_logistic_regression(question):\n",
        "    \n",
        "    q = question.split()\n",
        "    new_q = [word for word in q if word in word2vec_model.vocab]\n",
        "    \n",
        "    test_sample = np.mean(word2vec_model[new_q], axis=0)\n",
        "    \n",
        "    return model.predict(test_sample.reshape(1,-1))\n",
        "    "
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBwjBJoUb6X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49aa772b-4987-413e-801f-c61d0bf0e03e"
      },
      "source": [
        "# Evaluate the intent classifier\n",
        "# Your intent classifier performance will be close to 100 if you have done a good job.\n",
        "evaluate_intent_accuracy(predict_intent_using_logistic_regression)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AddToPlaylist 1.0 100\n",
            "GetWeather 1.0 100\n",
            "BookRestaurant 1.0 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnGNOHNXbSN"
      },
      "source": [
        "## Problem 2.3: Slot filling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXIMs6_b6X7"
      },
      "source": [
        "Build a slot filling model. We will just work with `AddToPlaylist` intent. Ignore other intents.\n",
        "\n",
        "Hint: No need to rely on machine learning here. You can use ideas like maximum string matching to identify which slots are active and what thier values are. This problem's solution is intentionally left underspecified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpg1x-qeb6X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a77d6cc-4686-4571-eb1d-cdba952d672f"
      },
      "source": [
        "# Let's stick to one target intent.\n",
        "target_intent = \"AddToPlaylist\"\n",
        "\n",
        "# This intent has the following slots\n",
        "target_intent_slot_names = set()\n",
        "for sample in train_data:\n",
        "    if sample['intent'] == target_intent:\n",
        "        for slot_name in sample['slots']:\n",
        "            target_intent_slot_names.add(slot_name)\n",
        "print(target_intent_slot_names)\n",
        "\n",
        "\n",
        "# Extract all the relevant questions of this target intent from the test examples.\n",
        "target_intent_questions = [] \n",
        "for i, question in enumerate(test_questions):\n",
        "    if test_answers[i]['intent'] == target_intent:\n",
        "        target_intent_questions.append(question)\n",
        "print(len(target_intent_questions))"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'music_item', 'entity_name', 'artist', 'playlist_owner', 'playlist'}\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_values():\n",
        "  music_item = []\n",
        "  entity_name = []\n",
        "  artist = []\n",
        "  playlist_owner = []\n",
        "  playlist = []\n",
        "\n",
        "  for sample in train_data:\n",
        "    if sample['intent'] == target_intent:\n",
        "      if 'music_item' in sample['slots']:\n",
        "        music_item.append(sample['slots']['music_item'])\n",
        "      if 'entity_name' in sample['slots']:\n",
        "        entity_name.append(sample['slots']['entity_name'])\n",
        "      if 'artist' in sample['slots']:\n",
        "        artist.append(sample['slots']['artist'])\n",
        "      if 'playlist_owner' in sample['slots']:\n",
        "        playlist_owner.append(sample['slots']['playlist_owner'])\n",
        "      if 'playlist' in sample['slots']:\n",
        "        playlist.append(sample['slots']['playlist'])\n",
        "\n",
        "\n",
        "  return  set(music_item), set(entity_name), (artist), set(playlist_owner), set(playlist)\n",
        "\n",
        "music_item, entity_name, artist, playlist_owner, playlist = extract_values()\n",
        "\n",
        "\n",
        "\n",
        "print(artist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyDeamxbXcnl",
        "outputId": "4e51ceb5-4f31-48d7-e86d-0db9f263cb07"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Manuelita la tortuga', 'warpaint', 'Metal Church', 'Kick Over the Traces', 'confessions', 'Falling Stars', 'The Greyest of Blue Skies', 'Transmission', 'moon river', 'Games', 'stefanie', 'heresy and the hotel choir', 'Hello Central, Give Me Heaven', 'List of Rush instrumentals', 'od mene se odvikavaj', 'Driven to Tears', 'once bitten twice bitten', 'sonntag', 'Strong', 'Stuck on Nothing', 'The Green Book', 'Kids of the Black Hole', 'kan', 'Georgetown University Alma Mater', 'ojuelegba', 'woman of the world', 'Not Fade Away', 'coming back to life', 'faccetta nera', 'Clapton Chronicles The Best of Eric Clapton', 'Corrina, Corrina', 'un jour dans notre vie', 'hammer', 'crying, waiting, hoping', 'As I Was Going to St Ives', 'Every Song Is a Cry for Love', 'cut the world', 'morton feldman piano and string quartet', 'A Journal of the Plague Year', 'Happy Holidays', 'go', 'the second three years', 'Indocumentado', 'A-Hunting We Will Go', 'vera', 'Tranquility', 'all the years', 'Give Us Rest', 'that was only yesterday the last ep', 'darkest angels', 'kids in the street', 'Sorrow', 'farruko presenta los menores', 'Teddy Boy', 'Chér', 'something so right', 'omnipresent', 'See Me Now', 'outside the dream syndicate', 'Run Rudolph Run', 'Waltz for Debby', 'new steps', 'hold my liquor', 'Spirit of Life', 'a shot at glory', 'The Broken Wave', 'count von cosels obsession', 'Unbelievable', 'Moribund the Burgermeister', '50 minute technicolor dream', 'Defined by Struggle', 'The Spine Surfs Alone', 'spanish castle magic', 'the man who never lied', 'no prejudice', 'Land of the Dead', 'Blue Feather', 'fusil contra fusil', 'dark days in paradise', 'the wee wee man', 'gn', 'Nana Mizuki Live Fighter -Blue x Red Side-', 'died', 'On with the Show', 'The Orange and the Green', 'Appreciation Day', 'Heart Like a Hurricane', 'r u still in 2 it', 'Super Turnt Up', 'Digital at Montreux', 'la jaula de oro', 'Sweet Black Angel', 'Bring Me Down', 'what if we were real', 'My Belgian Rose', 'Madman', 'SIR John Winston Ono Lennon', 'The Hellacopters singles', 'time flies 1994-2009', 'wreck of the tennessee gravy train', 'als het om de liefde gaat', 'shelter from the storm', 'Unite and Win', 'Cake to Bake', 'Brave and Crazy', 'The Best of Guitar Shorty', 'Hallucinations of Despair', 'hohenfriedberger marsch', '9th Inning', 'Strange Days', 'sons of the sea', 'Embryo', 'to anacreon in heaven', 'night and day', 'you put a move on my heart', 'el choclo', 'Hotter Than Hell', 'The Maid of Amsterdam', 'Shall We Dance', 'M-CABI', 'come on a cone', 'Hey, Johnnie Cope, Are Ye Waking Yet', 'The Muppet Show 2', 'deep purple', 'outlaw blues', 'what child is this', 'Disco Tango', 'lunacy', 'do you want to build a snowman', 'in person at carnegie hall', 'olympia 1959', 'Put Your Hand Inside the Puppet Head', 'jennie, jennie', 'bright and breezy', 'git', 'tunnel rats', 'Chamberlain Waits', 'Intelligence and Sacrifice', 'New Wave Blues', 'Of Rivers and Religion', 'D Generation', 'a new england', 'Aprite le finestre', 'what if punk never happened', 'Come on Feel the Lemonheads', 'the magnificent tree', 'how', 'Runaljod gap var Ginnunga', 'Me & the Rhythm', 'Milk', 'I Messed Up', '4813', 'dahmer', 'Have You Met Miss Jones', 'Be Yourself Tonight', 'Secrets of the Alibi', 'Liberty Forever', 'novena on a nocturn', 'i pledge allegiance to the grind', 'the essential jacksons', 'a great day for freedom', 'geminism', 'new page', 'Milas Poli', 'Baby Lemonade', 'colors of the wind', 'TEENAGER', 'Grey Cloudy Lies', 'refrain', 'Here Comes Santa Claus', 'push the button', 'already over, pt 2', 'Perfect Sense, Part I', 'when ice met cream', 'future', 'The Open Door', 'rock & roll', 'now the hits of winter 2008', 'ten green bottles', 'must b 21', 'Through the Darkness They March', 'iene miene mutte', 'horace silver and the jazz messengers', 'Heat of the Night', 'manuelita', 'Progress', 'share the well', 'journey to love', 'Jamaica Say You Will', 'joy division the complete bbc recordings', 'Tinker, Tailor', 'my hands', 'twin peaks fire walk with me', 'Księga Urodzaju', 'hopeful', 'hello i must be going', 'Unbound', 'villotta', 'munia the tale', 'great grape', 'Lullaby of Birdland', 'rev-raptor', 'Badmeaningood Volume4', 'Kloden drejer', 'national treasure book of secrets', 'undressed', 'suffer little children', 'donnie g don gorilla', 'little musgrave and the lady barnard', 'sugarolly days', 'I Almost Lost My Mind', 'Hind Etin', 'Tsūzetsu', 'my heart stood still', 'Book of Love', 'live with me', 'seek & destroy', 'Paper Doll', 'como un tatuaje', 'Curious corn', 'nothing remains the same', 'Welcome to the Cruel World', 'Barbara', '2012 Zwanzig Zwölf', 'Foreign Affair', 'Cater Fe She', 'Written in Red', 'spirit touches ground', 'Song about the Towel', 'shake your rump', 'vintage 74', 'Broken Hearted Melody', 'Visible Wings', 'the patty patty sound', 'the fire and the wind', 'disco', 'big generator', 'la voce', 'all together now', 'duett', 'Tunnel of Love', 'exorcising ghosts', 'Pause', 'When I Paint My Masterpiece', 'Hanging On', 'Marching Band', 'as with gladness men of old', 'the time warp', 'house of gold', 'Nightmares That Surface from Shallow Sleep', 'checkmate', 'ares', 'Fear and Bullets', 'wastedagain', 'Theater', 'Raise Your Fist', 'zombieland', 'Honey Hush', 'knocked out loaded', 'psycho', 'maschi e altri', '9AM in Dallas', 'satisfied', 'Metal Gear Solid 2 Sons of Liberty Soundtrack 2 The Other Side', 'Unconscious State', 'Third Stone from the Sun', 'all out of luck', 'Testifying', 'x forza e x amore', 'from the ashes', 'Asking for It', 'splendido hotel', '2120 south michigan avenue', 'Inca Roads', 'The Private Collection', 'anti ep', 'If I Could Be with You', 'Suus', 'silver apples', 'King of America', 'Don and Sherri', 'Exit', 'live in detroit', 'Sci-Fi Crimes', 'introducing a r rahman', 'Get Happy', 'there is nothing like a dame', 'The Stars and Stripes Forever', 'rmx works from cyber trance presents ayu trance 3', 'rise of the infidels', 'the 3rd world', 'pangaea', 'entre um coco e um adeus', 'Without Your Love', 'viderunt omnes', 'Kunnon syy', 'Rattus at the Roundhouse', 'Before the Eulogy', 'i dreamt of a dragon', 'xanadu', 'take it back', 'Volver', 'How', 'picasso baby', 'the unraveling', 'Jack Be Nimble', 'Famous', 'A Compilation of Warped Music II', 'Digital Line', 'Love Story wa Totsuzen ni', 'The 3rd World', 'So Far', 'Nothing Can Stop Us', 'histoire de melody nelson', 'Constructs of the State', 'grim skunk', 'golden boy', '21 AT 33', 'Badonviller Marsch', 'in my dreams', 'house of pain', 'Bring Back My Daddy to Me', 'Los Hombres Calientes Volume 3 New Congo Square', 'The Best Is Yet to Come', 'Ik Tara', 'even serpents shine', 'ramble on', 'Fair Charlotte', 'Ryō Yamazaki', 'Deus Deceptor', 'look to you', 'Always', 'changes & things', 'blood guts & glory', 'cliffs of dooneen', 'Banana Republic', 'the cave canem demos', 'the war is not over', 'moby grape live', 'Glory in the Highest A Christmas Record', 'tour generación rbd en vivo', 'madlib invazion', 'Halfway Between Here and There', 'The Rough Guide to the Music of Eastern Europe', 'Split the Difference', 'Czarna dziewczyna', 'A Bum Note and a Bead of Sweat', 'these are the days', 'country favorites willie nelson style', 'zamiast burzy', 'The Little Grey Mother Who Waits All Alone', 'maggie mcgill', 'No Mystery', 'fuzzy logic', 'Live from Aragon Ballroom', 'Spanish Harlem Incident', 'Por una cabeza', 'Born Free', 'circus', 'Me Kommeni Tin Anasa', 'lapponia', 'live and rare', 'a guy is a guy', 'the pop tarts', 'Ghost on the Dance Floor', 'elastic love', 'Roots of the Outsiders', 'perfecting loneliness', 'back at the velvet lounge', 'Sunday Express Live', 'shot forth self living', 'por tu maldito amor', 'hold tight', 'The Soul Sessions Volume 2', 'd-day dodgers', 'unwelcome', 'xsuie', 'steel guitar rag', 'global underground 006 sydney', 'i should have known better', 'Absolutely Sweet Marie', 'All Bad', 'Deuce', 'The Rebirth of Kirk Franklin', 'pacific ocean blues', 'iemand als jij', 'The Lamb Lies Down on Broadway', 'Pedro Navaja', 'El Noi de la Mare', 'In the Mood', 'First Issue', 'agua y sal', 'falling upstairs', 'Contact', 'polka medley', 'Bien Acompañado', 'East Grand Blues', 'live around the world', 'eternally', 'I’m Only a Man', 'Por una Cabeza', 'Black Ribbons', 'Us Placers', 'extended play', 'If You Were Mine', 'the past behind', 'Day After Day', 'Sin rencor', 'When You Come Back', 'Star Light, Star Bright', 'Vyechnyy strannik', 'cry like a baby', 'mon beau sapin', 'Showcase in a Suitcase', 'Tarte', 'Ultimatum', 'scarlet begonias', 'This Generation', '100% te ljubam', 'covenant', 'beijing huanying ni', 'Monster Monster', 'avant que l’ombre à bercy', 'Bossa Nova Soul Samba', 'lady maisry', 'children of telepathic experiences', 'jtr', 'I Hate Myself and I Want to Die', 'Paloma negra', 'Molly and Tenbrooks', 'bill evans', 'natasha', 'inconfundible', 'Mayya', 'Shangri-La', 'unreachable', 'El Viaje de Copperpot', 'out of the air', 'Geschwisterliebe', 'take up thy stethoscope and walk', 'Beside You', 'Club Mix', 'decade in the sun best of stereophonics', 'Dying Mapa I', 'Ayumi Hamasaki Arena Tour 2009 A Next Level', 'Irish Heartbeat', 'Maria Magdalena', 'Revolution Revolución', 'banking violence and the inner life today', 'genocide', 'A Gate Through Bloodstained Mirrors', 'Candlelight', 'Der K und K Kalypso aus Wien', 'Sonntagskind', 'janie jones', 'GodMusic', 'best of uetoaya', 'The Blurred Crusade', 'Framed', 'parempi mies', 'The Block Brochure Welcome to the Soil 6', 'let the season in', 'the ragged curtain', 'slave to the rhythm', 'Butterfly House', 'LA Woman', 'doing all right', 'Eternal Prisoner', 'bad news', 'the happy elf', 'Zos Kia Cultus', 'saxophone supremacy', 'NIB', 'dear hearts and gentle people', 'Opus de Funk', 'De Principii Evangelikum', 'four in blue', 'concrete roots', 'Omoide wa Okkusenman', 'Up to the Mountain', 'Please, Please, Please, Let Me Get What I Want', 'however much i booze', 'keine grenzen', '157 Riverside Avenue', 'Visjoner', 'one and only', 'tire me', 'The “Chirping” Crickets', 'glam', 'until the end of time', 'antisleep volume 04', 'Halley', 'Quicksand', 'Puzzles Like You', 'Do You Love Me', 'United Abominations', 'child owlet', 'people take pictures of each other', '3 Natsu Natsu Mini Berryz', 'freddie freeloader', 'Nothing Fancy', 'blaydon races', 'Toi', 'repent replenish repeat', 'say it ain’t so', 'back porch spirituals', 'Falcon', 'Hello, My Lover, Goodbye', 'shame on you', 'Warm and Beautiful', 'Scarred', 'venedig im regen', 'impossible is nothing', 'My Very Best', 'potje met vet', 'the silence', 'Esquivando charcos', 'Smokefree RockQuest 2005', 'sleeping with ghosts', 'Take Me Back to Dear Old Blighty', 'Firehouse', 'used to love her', 'karusellen', 'singing in the trees', 'my old kentucky home', 'Step to Me', 'Fra Mols til Skagen', 'Mariem Hassan con Leyoad', 'willie nelson live', 'monsters', 'Limey', 'Chrome Reflection', 'teriazume', 'Blood On The Face', 'back on the dancefloor', 'the crabfish', 'qriii', 'riddim driven engine 54 & humanity', 'Keep Your Receipt EP', 'marcia carolus rex', 'la la', 'feel the passion', 'Cream', 'in the heart of the world', 'The Little House We Built', 'Hatfield and the North', 'Rak biszewilo', 'flying', 'love love love', 'One Good Reason', 'Warning Device', 'Indestructible', 'your loving flame', 'image', 'histoires sans paroles', 'consequences', 'Robin Hood and Queen Katherine', 'Sveta ljubav', 'Raisins secs et amandes', 'Family Tree', 'cross bones style', 'Coordinates of Confusion', 'the secret wars', 'Partners in Crime', 'El valle del Jarama', 'john brown', 'time warp', 'Nazad, nazad, Kalino mome', 'radhae unakku kobam aagathadi', 'Días Felices', 'lay down your arms', 'Joplin in Concert', 'Om', 'det kimer nu til julefest', 'Devil Pray', 'no more sorrow', 'The Darkest Red', '12 Odd Future Songs', 'Mary Wells Sings My Guy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'I love you'\n",
        "s.find('I love')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFEJYiIpaRrx",
        "outputId": "f16049da-bef8-4364-9cfd-03c7be0ceef2"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7_ldSKob6X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cea9a0-d014-4b55-a7ba-e89700c5ebcd"
      },
      "source": [
        "def initialize_slots():\n",
        "    slots = {}\n",
        "    for slot_name in target_intent_slot_names:\n",
        "        slots[slot_name] = None\n",
        "    return slots\n",
        "\n",
        "def predict_slot_values(question):\n",
        "    question = question.lower()\n",
        "    slots = initialize_slots()  \n",
        "    \n",
        "    for slot_name in target_intent_slot_names:\n",
        "      \n",
        "      if slot_name == 'music_item':\n",
        "        for value in music_item:\n",
        "          if question.find(str(value)) != -1:\n",
        "            slots[slot_name] = value \n",
        "            \n",
        "          \n",
        "     \n",
        "     \n",
        "      elif slot_name == 'entity_name':\n",
        "        \n",
        "        for value in entity_name:\n",
        "          if question.find(str(value).lower()) != -1 :\n",
        "            slots[slot_name] = value \n",
        "            \n",
        "            \n",
        "      elif slot_name == 'artist':\n",
        "        \n",
        "        for value in artist:\n",
        "          if question.find(str(value).lower()) != -1:\n",
        "            slots[slot_name] = value \n",
        "            \n",
        "          \n",
        "      elif slot_name == 'playlist_owner':\n",
        "        \n",
        "        for value in playlist_owner:\n",
        "          if question.find(value.lower()) != -1:\n",
        "            slots[slot_name] = value\n",
        "             \n",
        "            \n",
        "      \n",
        "      else:\n",
        "        \n",
        "        for value in playlist:\n",
        "          if question.find(value.lower()) != -1:\n",
        "            slots[slot_name] = value \n",
        "            \n",
        "            \n",
        "        \n",
        "      \n",
        "    return slots\n",
        "\n",
        "def evaluate_slot_prediction_recall(slot_prediction_function):\n",
        "    correct = Counter()\n",
        "    total = Counter()\n",
        "    # predict slots for each question\n",
        "    for i, question in enumerate(target_intent_questions):\n",
        "        i = test_questions.index(question) # This line is added after the assignment release\n",
        "        gold_slots = test_answers[i]['slots']\n",
        "        predicted_slots = slot_prediction_function(question)\n",
        "        for name in target_intent_slot_names:\n",
        "            if name in gold_slots:\n",
        "                total[name] += 1.0\n",
        "                if predicted_slots.get(name, None) != None and predicted_slots.get(name).lower() == gold_slots.get(name).lower(): # This line is updated after the assignment release\n",
        "                    correct[name] += 1.0\n",
        "    for name in target_intent_slot_names:\n",
        "        print(f\"{name}: {correct[name] / total[name]}\")\n",
        "\n",
        "\n",
        "# Our reference implementation got these numbers. You can ask others on Slack what they got.\n",
        "# music_item 1.0\n",
        "# playlist 0.67\n",
        "# artist  0.021739130434782608\n",
        "# playlist_owner 0.9444444444444444\n",
        "# entity_name 0.1111111111111111\n",
        "print(\"Slot accuracy for your slot prediction model\")\n",
        "evaluate_slot_prediction_recall(predict_slot_values)\n"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slot accuracy for your slot prediction model\n",
            "music_item: 1.0\n",
            "entity_name: 0.05555555555555555\n",
            "artist: 0.13043478260869565\n",
            "playlist_owner: 0.9444444444444444\n",
            "playlist: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm4reFpdb6X8"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUo4NblMb6X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e685cee8-8a7c-446b-fa77-121a6646ccda"
      },
      "source": [
        "\n",
        "\n",
        "def TP(slot_name, slot_prediction_function=predict_slot_values):\n",
        "  correct = 0.\n",
        "  total = 0.\n",
        "  for i,question in enumerate(target_intent_questions):\n",
        "      pred = slot_prediction_function(question)[slot_name]\n",
        "      if slot_name in test_answers[i]['slots']:\n",
        "        total += 1\n",
        "        if pred != None:\n",
        "          correct += 1\n",
        "\n",
        "  return correct / total\n",
        "\n",
        "for name in target_intent_slot_names:\n",
        "  print('TP:\\t' , name, TP(name))\n",
        "\n",
        "\n"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP:\t music_item 0.5714285714285714\n",
            "TP:\t entity_name 0.5\n",
            "TP:\t artist 0.07142857142857142\n",
            "TP:\t playlist_owner 0.6470588235294118\n",
            "TP:\t playlist 0.9705882352941176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def FP(slot_name, slot_prediction_function=predict_slot_values):\n",
        "  false = 0.\n",
        "  total = 0.\n",
        "  for i,question in enumerate(target_intent_questions):\n",
        "      pred = slot_prediction_function(question)[slot_name]\n",
        "      if slot_name not in test_answers[i]['slots']:\n",
        "        total += 1\n",
        "        if pred != None:\n",
        "          false += 1\n",
        "  \n",
        "  return false / total\n",
        "\n",
        "\n",
        "\n",
        "for name in target_intent_slot_names:\n",
        "  print('FP:\\t' , name, FP(name))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5FHXdoKpR19",
        "outputId": "2613a595-34db-4503-b2a8-f670f33f7a36"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP:\t music_item 0.6329113924050633\n",
            "FP:\t entity_name 0.13829787234042554\n",
            "FP:\t artist 0.05813953488372093\n",
            "FP:\t playlist_owner 0.5060240963855421\n",
            "FP:\t playlist 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQjb1-TCb6X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdee7e8-a865-40c1-a825-cdaa039e822c"
      },
      "source": [
        "def TN(slot_name, slot_prediction_function=predict_slot_values):\n",
        "  correct = 0.\n",
        "  total = 0.\n",
        "  for i,question in enumerate(target_intent_questions):\n",
        "      pred = slot_prediction_function(question)[slot_name]\n",
        "      if slot_name not in test_answers[i]['slots']:\n",
        "        total += 1\n",
        "        if pred == None:\n",
        "          correct += 1\n",
        "  return correct/total\n",
        "\n",
        "for name in target_intent_slot_names:\n",
        "  print('TN:\\t' , name, TN(name))\n",
        "  \n"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TN:\t music_item 0.3670886075949367\n",
            "TN:\t entity_name 0.8617021276595744\n",
            "TN:\t artist 0.9418604651162791\n",
            "TN:\t playlist_owner 0.4939759036144578\n",
            "TN:\t playlist 0.09090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJHTfEMqb6X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbe6b6c-c8b6-473c-88aa-5448a2ae6dfb"
      },
      "source": [
        "# Find a false negative prediction for each slot\n",
        "# Fill in your code below along with a print statement\n",
        "def FP(slot_name, slot_prediction_function=predict_slot_values):\n",
        "  false = 0.\n",
        "  total = 0.\n",
        "  for i,question in enumerate(target_intent_questions):\n",
        "      pred = slot_prediction_function(question)[slot_name]\n",
        "      if slot_name in test_answers[i]['slots']:\n",
        "        total +=1\n",
        "        if pred == None:\n",
        "          false += 1\n",
        "  return false/total\n",
        "\n",
        "for name in target_intent_slot_names:\n",
        "  print('TN:\\t' , name, FP(name))"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TN:\t music_item 0.42857142857142855\n",
            "TN:\t entity_name 0.5\n",
            "TN:\t artist 0.9285714285714286\n",
            "TN:\t playlist_owner 0.35294117647058826\n",
            "TN:\t playlist 0.029411764705882353\n"
          ]
        }
      ]
    }
  ]
}